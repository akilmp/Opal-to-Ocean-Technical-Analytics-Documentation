{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "048cdaae",
   "metadata": {},
   "source": [
    "# 02 \u2014 Explore results and what-ifs\n",
    "\n",
    "This exploratory notebook evaluates the trained model's behaviour, visualises distributional trends, and applies scenario analyses using the synthetic mart rebuilt via `make ingest && make marts && make analyze` with confidence intervals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\ud83d\udca1 **Refresh the marts before exploring**\n",
    "\n",
    "```bash\n",
    "make ingest && make marts && make analyze\n",
    "```\n",
    "\n",
    "This sequence regenerates the synthetic raw inputs, rebuilds `data/marts/fact_day.parquet`, and updates the scenario adjustments consumed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bae9e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a555a87",
   "metadata": {},
   "source": [
    "## Load marts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a51ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT = Path(os.environ.get('OPAL_OCEAN_PROJECT_ROOT', Path.cwd()))\n",
    "MARTS_DIR = Path(os.environ.get('OPAL_OCEAN_MARTS_DIR', PROJECT_ROOT / 'data' / 'marts'))\n",
    "FACT_DAY_PATH = MARTS_DIR / 'fact_day.parquet'\n",
    "SCENARIO_PATH = MARTS_DIR / 'what_if_scenarios.csv'\n",
    "if not FACT_DAY_PATH.exists():\n",
    "    raise FileNotFoundError('Run `make ingest && make marts && make analyze` to create data/marts/fact_day.parquet.')\n",
    "if not SCENARIO_PATH.exists():\n",
    "    raise FileNotFoundError('Run `make analyze` to refresh data/marts/what_if_scenarios.csv.')\n",
    "\n",
    "df = pd.read_parquet(FACT_DAY_PATH)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "if 'beach_ok' in df.columns:\n",
    "    df['beach_ok'] = df['beach_ok'].astype(int)\n",
    "scenarios = pd.read_csv(SCENARIO_PATH)\n",
    "expected_columns = {\n",
    "    'scenario', 'description', 'delta_commute_minutes', 'delta_reliability',\n",
    "    'delta_pm25_mean', 'delta_rain_24h_mm', 'delta_steps', 'delta_sleep_hours', 'delta_caffeine_mg'\n",
    "}\n",
    "missing = expected_columns.difference(scenarios.columns)\n",
    "if missing:\n",
    "    raise ValueError(f'Scenario file missing columns: {sorted(missing)}')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c0ceb8f",
   "metadata": {},
   "source": [
    "## Daily trends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f1a475",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_summary = (\n",
    "    df.assign(weekday=df['weekday'].astype('category'))\n",
    "      .groupby('weekday')\n",
    "      [['mood_1_5', 'commute_minutes', 'opal_cost', 'reliability', 'pm25_mean', 'rain_24h_mm', 'steps', 'sleep_hours', 'caffeine_mg']]\n",
    "      .mean()\n",
    "      .sort_index()\n",
    ")\n",
    "daily_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7dc0c5d",
   "metadata": {},
   "source": [
    "The averages show how commute, environment, and activity markers align with the weekday rhythm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8039b005",
   "metadata": {},
   "source": [
    "## Refit the pipeline for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ff3c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['weekday', 'commute_minutes', 'opal_cost', 'reliability', 'pm25_mean', 'rain_24h_mm', 'beach_ok', 'steps', 'sleep_hours', 'caffeine_mg']\n",
    "target = 'mood_1_5'\n",
    "X = df[features]\n",
    "y = df[target]\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), ['weekday']),\n",
    "        ('num', StandardScaler(), ['commute_minutes', 'opal_cost', 'reliability', 'pm25_mean', 'rain_24h_mm', 'beach_ok', 'steps', 'sleep_hours', 'caffeine_mg']),\n",
    "    ]\n",
    ")\n",
    "model = Pipeline(steps=[('prep', preprocessor), ('regressor', LinearRegression())])\n",
    "model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03108611",
   "metadata": {},
   "source": [
    "## Confidence intervals for recent days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5209c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_interval(pipeline, features, target, samples=300, alpha=0.05, random_state=13):\n",
    "    rng = np.random.default_rng(random_state)\n",
    "    preds = []\n",
    "    for _ in range(samples):\n",
    "        idx = rng.integers(0, len(features), len(features))\n",
    "        X_s = features.iloc[idx]\n",
    "        y_s = target.iloc[idx]\n",
    "        pipeline.fit(X_s, y_s)\n",
    "        preds.append(pipeline.predict(features))\n",
    "    preds = np.vstack(preds)\n",
    "    point = pipeline.fit(features, target).predict(features)\n",
    "    lower = np.percentile(preds, 100 * alpha / 2, axis=0)\n",
    "    upper = np.percentile(preds, 100 * (1 - alpha / 2), axis=0)\n",
    "    return point, lower, upper\n",
    "\n",
    "window = min(7, len(df))\n",
    "recent = df.tail(window).copy()\n",
    "point, lower, upper = bootstrap_interval(model, X, y)\n",
    "recent['prediction'] = point[-window:]\n",
    "recent['lower_ci'] = lower[-window:]\n",
    "recent['upper_ci'] = upper[-window:]\n",
    "recent[['date', 'weekday', 'mood_1_5', 'prediction', 'lower_ci', 'upper_ci']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5a7e62",
   "metadata": {},
   "source": [
    "## Scenario-based sensitivity analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7207e265",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = df[features].mean(numeric_only=True).to_dict()\n",
    "baseline['weekday'] = df['weekday'].mode().iat[0]\n",
    "baseline_df = pd.DataFrame([baseline])\n",
    "baseline_prediction = model.predict(baseline_df)[0]\n",
    "scenario_rows = []\n",
    "for _, row in scenarios.iterrows():\n",
    "    scenario_features = baseline_df.copy()\n",
    "    scenario_features['commute_minutes'] += row['delta_commute_minutes']\n",
    "    scenario_features['reliability'] = (scenario_features['reliability'] + row['delta_reliability']).clip(0.0, 1.0)\n",
    "    scenario_features['pm25_mean'] += row['delta_pm25_mean']\n",
    "    scenario_features['rain_24h_mm'] += row['delta_rain_24h_mm']\n",
    "    scenario_features['steps'] += row['delta_steps']\n",
    "    scenario_features['sleep_hours'] += row['delta_sleep_hours']\n",
    "    scenario_features['caffeine_mg'] += row['delta_caffeine_mg']\n",
    "    prediction = model.predict(scenario_features)[0]\n",
    "    scenario_rows.append({\n",
    "        'scenario': row['scenario'],\n",
    "        'description': row['description'],\n",
    "        'predicted_mood': prediction,\n",
    "        'delta_vs_baseline': prediction - baseline_prediction\n",
    "    })\n",
    "scenario_results = pd.DataFrame(scenario_rows)\n",
    "scenario_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4a00834",
   "metadata": {},
   "source": [
    "Scenario deltas summarise how commute, reliability, air quality, and personal routines influence predicted mood."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}